<!DOCTYPE HTML>
<html>
<head>
    <title>Pneumonia Detection</title>
</head>
<body>
    <h1>Building a model</h1>
    <pre>
@author: ABI
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, plot_confusion_matrix


# Set the paths to your pneumonia and normal image directories
pneumonia_dir = r"C:\Users\ABI PRIYANKA\Downloads\pnuemonia\train\PNEUMONIA"
normal_dir = r"C:\Users\ABI PRIYANKA\Downloads\pnuemonia\train\NORMAL"

# Set the image dimensions
img_width, img_height = 150, 150

# Function to load and preprocess images
def load_images(path, label):
    images = []
    labels = []
    for image_file in os.listdir(path):
        if image_file.endswith('.jpg') or image_file.endswith('.jpeg'):
            image = cv2.imread(os.path.join(path, image_file))
            image = cv2.resize(image, (img_width, img_height))
            images.append(image)
            labels.append(label)
    return images, labels

# Load pneumonia images
pneumonia_images, pneumonia_labels = load_images(pneumonia_dir, 1)
# Downsample the pneumonia class
pneumonia_indices = np.random.choice(len(pneumonia_images), size=int(len(pneumonia_images) / 2), replace=False)
pneumonia_images = [pneumonia_images[i] for i in pneumonia_indices]
pneumonia_labels = [pneumonia_labels[i] for i in pneumonia_indices]

# Load normal images
normal_images, normal_labels = load_images(normal_dir, 0)

# Upsample the normal class
normal_indices = np.random.choice(len(normal_images), size=len(pneumonia_images), replace=True)
normal_images = [normal_images[i] for i in normal_indices]
normal_labels = [normal_labels[i] for i in normal_indices]

# Combine images and labels
images = pneumonia_images + normal_images
labels = pneumonia_labels + normal_labels


# Convert images and labels to NumPy arrays
images = np.array(images)
labels = np.array(labels)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Flatten the image data
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Preprocess the image data (optional: scale the pixel values to [0, 1])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(x_train, y_train)
rf_predictions = rf_classifier.predict(x_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_confusion_matrix = confusion_matrix(y_test, rf_predictions)
print("Random Forest Accuracy:", rf_accuracy)
print("Random Forest Confusion Matrix:")
print(rf_confusion_matrix)



                 

# Calculate the square root of the number of samples
sqrt_n = int(np.sqrt(x_train.shape[0]))

# Create and train the KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=sqrt_n)

knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(x_train, y_train)
knn_predictions = knn_classifier.predict(x_test)
knn_accuracy = accuracy_score(y_test, knn_predictions)
knn_confusion_matrix = confusion_matrix(y_test, knn_predictions)
print("K-Nearest Neighbors Accuracy:", knn_accuracy)
print("K-Nearest Neighbors Confusion Matrix:")
print(knn_confusion_matrix)


# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(x_train, y_train)
dt_predictions = dt_classifier.predict(x_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
dt_confusion_matrix = confusion_matrix(y_test, dt_predictions)
print("Decision Tree Accuracy:", dt_accuracy)
print("Decision Tree Confusion Matrix:")
print(dt_confusion_matrix)

# Plotting the Confusion Matrix
def plot_confusion_matrix(cm, labels):
    df_cm = pd.DataFrame(cm, index=labels, columns=labels)
    plt.figure(figsize=(8, 6))
    sns.set(font_scale=1.4)
    sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

# Plotting the Random Forest Confusion Matrix
plot_confusion_matrix(rf_confusion_matrix, ['Normal', 'Pneumonia'])

# Plotting the K-Nearest Neighbors Confusion Matrix
plot_confusion_matrix(knn_confusion_matrix, ['Normal', 'Pneumonia'])

# Plotting the Decision Tree Confusion Matrix
plot_confusion_matrix(dt_confusion_matrix, ['Normal', 'Pneumonia'])


# Bar plot of accuracy scores
models = ['Random Forest', 'K-Nearest Neighbors', 'Decision Tree']
accuracy_scores = [rf_accuracy, knn_accuracy, dt_accuracy]

plt.figure(figsize=(8, 6))
plt.bar(models, accuracy_scores, color='skyblue')
plt.xlabel('Classification Models')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Scores of Classification Models')
plt.ylim([0, 1])
plt.show()

-------------------------------------------------------------------------------------------------------

     <h1>TRAINING</h1>



from __future__ import print_function, division
import torch
import torch.nn as nn
import torch.optim as optim
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy
from PIL import Image
from sklearn.metrics import confusion_matrix

cudnn.benchmark = True
plt.ion()  # Interactive mode

transformers = {
    'train_transforms': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomRotation(20),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    'test_transforms': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    'valid_transforms': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
}

trans = list(transformers.keys())

path = 'C:/Users/ABI PRIYANKA/Downloads/pnuemonia'
categories = ['train', 'valid', 'test']

dset = {
    x: datasets.ImageFolder(
        os.path.join(path, x),
        transform=transformers[trans[i]]
    )
    for i, x in enumerate(categories)
}




from sklearn.utils import resample

# Separate data for upsampling and downsampling
train_data = dset['train'].samples.copy()
train_targets = dset['train'].targets.copy()


# Convert train_targets to a numpy array
train_targets = np.array(train_targets)

# Show the number of images in each class
print("Original Train Dataset:")
print("Normal class count:", np.bincount(train_targets.astype(int))[0])
print("Pneumonia class count:", np.bincount(train_targets.astype(int))[1])
print()


# Upsample "NORMAL" class
normal_indices = np.where(train_targets == 0)[0]
upsampled_normal_indices = resample(normal_indices, replace=True, n_samples=len(normal_indices) * 2, random_state=42)

# Downsample "PNEUMONIA" class
pneumonia_indices = np.where(train_targets == 1)[0]
downsampled_pneumonia_indices = resample(pneumonia_indices, replace=False, n_samples=len(pneumonia_indices) // 2, random_state=42)

# Combine the upsampled "NORMAL" class and downsampled "PNEUMONIA" class
resampled_indices = np.concatenate((upsampled_normal_indices, downsampled_pneumonia_indices))

# Create a new train dataset with balanced classes
resampled_data = [train_data[i] for i in resampled_indices]
resampled_targets = train_targets[resampled_indices]



print("Resampled Train Dataset:")
print("Normal class count:", np.bincount(resampled_targets.astype(int))[0])
print("Pneumonia class count:", np.bincount(resampled_targets.astype(int))[1])


dataset_sizes = {x: len(dset[x]) for x in categories}

# Update the 'train' dataset with resampled data
dset['train'].samples = resampled_data
dset['train'].targets = resampled_targets
dataset_sizes['train'] = len(dset['train'])


num_threads = 4

dataloaders = {
    x: torch.utils.data.DataLoader(
        dset[x], batch_size=64, shuffle=True, num_workers=num_threads
    )
    for x in categories
}
def imshow(inp, title=None):
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)
    
inputs, classes = next(iter(dataloaders["train"]))
out = torchvision.utils.make_grid(inputs)
class_names = dset["train"].classes
imshow(out, title=[class_names[x] for x in classes])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train_model(model, criterion, optimizer, scheduler, num_epochs=5):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    train_losses = []
    valid_losses = []
    train_accs = []
    valid_accs = []
    true_labels = []
    predicted_labels = []
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch + 1, num_epochs))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()  # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass
                # Track history if only in train phase
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # Backward pass and optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # Statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                
                true_labels.extend(labels.data.tolist())
                predicted_labels.extend(preds.tolist())

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            if phase == 'train':
                train_losses.append(epoch_loss)
                train_accs.append(epoch_acc)
            else:
                valid_losses.append(epoch_loss)
                valid_accs.append(epoch_acc)

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            # Deep copy the model if it's the best so far
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                torch.save(best_model_wts, 'good_model.pth')


        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best validation accuracy: {:4f}'.format(best_acc))

    # Load best model weights
    model.load_state_dict(best_model_wts)
    
    # Compute confusion matrix
    cm = confusion_matrix(true_labels, predicted_labels)
    
    class_names = ['Normal', 'Pneumonia']
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()
    return model, train_losses, valid_losses, train_accs, valid_accs

# Load a pre-trained ResNet model
model = models.resnet18(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, len(class_names))
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

# Train the model
model, train_losses, valid_losses, train_accs, valid_accs = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=5)





# Save the model with the best validation accuracy
best_model_filename = 'good_model.pth'
torch.save(model.state_dict(), best_model_filename)
print("Best model saved as:", best_model_filename)



    </pre>
</body>
</html>

